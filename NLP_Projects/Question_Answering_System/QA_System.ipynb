{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a794f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter Your Question Now: who is michael jackson\n",
      "Answer:  michael jackson is american singer \n",
      "Please Enter Your Question Now: where is fairfax\n",
      "Answer:  fairfax is in united states \n",
      "Please Enter Your Question Now: when is christmas\n",
      "Answer:  christmas is on 24th december \n",
      "Please Enter Your Question Now: what is hyderabad\n",
      "Answer:  hyderabad is old city \n",
      "Please Enter Your Question Now: how do you know me\n",
      "Sorry, we cannot answer your question at the moment\n",
      "Please Enter Your Question Now: exit\n",
      "Thank you, Bye\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "import re\n",
    "import wikipedia\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "\n",
    "\n",
    "# to remove the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Cleaning data here \n",
    "# Tokenizing, removing punctuation and stop words\n",
    "# also removing the rewrite string from the search result\n",
    "def clean_data(rewrite_string, data):\n",
    "    data = data.lower()\n",
    "    s = data.replace(rewrite_string, \"\")\n",
    "    for i in rewrite_string.split(\" \"):\n",
    "        if (i not in stop_words):\n",
    "            s = s.replace(i, \"\")\n",
    "    tokenized_text = word_tokenize(s)\n",
    "    without_punct = [''.join(\n",
    "        eachcharac for eachcharac in eachword if eachcharac not in string.punctuation) for\n",
    "        eachword in tokenized_text]\n",
    "    without_stop_words = \"\"\n",
    "    for i in without_punct:\n",
    "        if i not in stop_words:\n",
    "            without_stop_words = without_stop_words + \" \" + i\n",
    "    return without_stop_words\n",
    "\n",
    "# Searching for results on wikipedia\n",
    "def get_content_from_wiki(list_of_rewrittenqs_with_wts):\n",
    "    content_weight_rewrite = []\n",
    "    for j in list_of_rewrittenqs_with_wts:\n",
    "        res = \"\\\"\" + j[\"rewrite_string\"] + \"\\\"\"\n",
    "        wiki_res = wikipedia.search(res)\n",
    "        # print(wiki_res)\n",
    "        super_string = \"\"\n",
    "        for i in wiki_res:\n",
    "            # some results had this error so we had to remove disambiguation\n",
    "            if (not re.match(\".*disambiguation.*\", i)):\n",
    "                tokenized_sentence = []\n",
    "                result = \"\"\n",
    "                try:\n",
    "                    result = wikipedia.summary(i)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                result_string = result\n",
    "                super_string = super_string + \" \" + clean_data(j[\"rewrite_string\"], result_string)\n",
    "                # Forming a list of dictionaries containing \n",
    "                # the rewrite string, \n",
    "                # content from the wikipedia page\n",
    "                # weight given to each result\n",
    "        content_weight_rewrite.append(\n",
    "            {\n",
    "                \"statement\": j[\"rewrite_string\"],\n",
    "                \"content\": super_string,\n",
    "                \"weight\": j[\"priority\"]\n",
    "            }\n",
    "        )\n",
    "        super_string = \"\"\n",
    "\n",
    "    return content_weight_rewrite\n",
    "\n",
    "# to rewrite the input queries\n",
    "def rewrite_ques_assign_weights(question):\n",
    "    sample_txt = question\n",
    "    list_of_dicts_with_weights = []\n",
    "    \n",
    "    # WHO condition\n",
    "    if re.match(r'^[wW]ho', sample_txt):\n",
    "        match_res = re.match('^[wW]ho (is|was|[a-z]*) (.*)', sample_txt)\n",
    "        if (match_res.groups()[0] == 'is' or match_res.groups()[0] == 'was'):\n",
    "            list_of_dicts_with_weights.append(\n",
    "                {\n",
    "                    \"rewrite_string\": str(match_res.groups()[1]) + ' ' + str(match_res.groups()[0]),\n",
    "                    \"priority\": 5\n",
    "                })\n",
    "            list_of_dicts_with_weights.append(\n",
    "                {\n",
    "                    \"rewrite_string\": str(match_res.groups()[1]) + ' ' + str(match_res.groups()[0] + \" known for\"),\n",
    "                    \"priority\": 3})\n",
    "            list_of_dicts_with_weights.append(\n",
    "                {\n",
    "                    \"rewrite_string\": str(match_res.groups()[1]) + ' ' + str(match_res.groups()[0] + \" famous for\"),\n",
    "                    \"priority\": 2})\n",
    "        else:\n",
    "            res = str(match_res.groups()[1]) + ' was ' + str(match_res.groups()[0] + ' by ')\n",
    "\n",
    "    # WHEN condition\n",
    "    if re.match(r'^[wW]hen', sample_txt):\n",
    "        match_res = re.match('^[wW]hen (is|was) (.*)', sample_txt)\n",
    "        if (match_res.groups()[0] == 'is' or match_res.groups()[0] == 'was'):\n",
    "            list_of_dicts_with_weights.append(\n",
    "                {\n",
    "                    \"rewrite_string\": str(match_res.groups()[1]) + ' ' + str(match_res.groups()[0] + ' on'),\n",
    "                    \"priority\": 5\n",
    "                })\n",
    "    \n",
    "    # WHAT condition\n",
    "    if re.match(r'^[wW]hat', sample_txt):\n",
    "        match_res = re.match('^[wW]hat (is|was) (.*)', sample_txt)\n",
    "        if (match_res.groups()[0] == 'is' or match_res.groups()[0] == 'was'):\n",
    "            list_of_dicts_with_weights.append(\n",
    "                {\n",
    "                    \"rewrite_string\": str(match_res.groups()[1]) + ' ' + str(match_res.groups()[0]),\n",
    "                    \"priority\": 5\n",
    "                })\n",
    "\n",
    "    # WHERE condition\n",
    "    if re.match(r'^[wW]here', sample_txt):\n",
    "        match_res = re.match('^[wW]here (is|was) (.*)', sample_txt)\n",
    "        if (match_res.groups()[0] == 'is' or match_res.groups()[0] == 'was'):\n",
    "            list_of_dicts_with_weights.append(\n",
    "                {\n",
    "                    \"rewrite_string\": str(match_res.groups()[1]) + ' ' + str(match_res.groups()[0]) + ' in',\n",
    "                    \"priority\": 5\n",
    "                })\n",
    "    return list_of_dicts_with_weights\n",
    "\n",
    "# Generating n-grams \n",
    "def gen_ngrams(text):\n",
    "    temp = []\n",
    "    u_grams = ngrams(word_tokenize(text.lower()), 1)\n",
    "    b_grams = ngrams(word_tokenize(text.lower()), 2)\n",
    "    t_grams = ngrams(word_tokenize(text.lower()), 3)\n",
    "    f_grams = ngrams(word_tokenize(text.lower()), 4)\n",
    "    fi_grams = ngrams(word_tokenize(text.lower()), 5)\n",
    "    #    for i in u_grams:\n",
    "    #        temp.append(i);\n",
    "    for i in b_grams:\n",
    "        temp.append(i);\n",
    "    for i in t_grams:\n",
    "        temp.append(i);\n",
    "    return FreqDist(temp).most_common(1)\n",
    "\n",
    "\n",
    "# This is where the program begins\n",
    "exit_flag = False\n",
    "while (not exit_flag):\n",
    "    user_input = input(\"Please Enter Your Question Now: \")\n",
    "    if ((\"who\" in user_input) or (\"when\" in user_input) or (\"where\" in user_input) or (\"what\" in user_input)):\n",
    "        can_do = 1\n",
    "    elif (\"exit\" in user_input):\n",
    "        can_do = 2\n",
    "    else:\n",
    "        can_do = 3\n",
    "\n",
    "    if (can_do == 1):\n",
    "        value_dicts = rewrite_ques_assign_weights(user_input)\n",
    "        similar_content_dict = get_content_from_wiki(value_dicts)\n",
    "        new_diction = {}\n",
    "        answer_dict = []\n",
    "        for i in similar_content_dict:\n",
    "            ans = gen_ngrams(i[\"content\"])\n",
    "            if (len(ans) != 0):\n",
    "                output = \"\"\n",
    "                for j in ans[0][0]:\n",
    "                    output = output + j + \" \"\n",
    "                answer_dict.append({\n",
    "                    \"answer_statement\": i[\"statement\"] + \" \" + output,\n",
    "                    \"weighted_avg\": ans[0][1] * i[\"weight\"]\n",
    "                })\n",
    "        #print(answer_dict)\n",
    "        for i in answer_dict:\n",
    "            print('Answer: ', i['answer_statement'])\n",
    "            \n",
    "    elif (can_do == 2):\n",
    "        print(\"Thank you, Bye\")\n",
    "        exit_flag = True\n",
    "        \n",
    "    else:\n",
    "        print('Sorry, we cannot answer your question at the moment')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9396ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
